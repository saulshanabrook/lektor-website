{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [C4ML - 2019](https://web.archive.org/web/20190218200248/https://www.c4ml.org/)\n",
    "\n",
    "I attended the innagural \"Compilers for Machine Learning\" workshop this year and want to collect my thoughts to process them and share them with others.\n",
    "\n",
    "I will go through each talk and then present overall findings. \n",
    "\n",
    "## Talks\n",
    "### Julia\n",
    "\n",
    "We (the PL/compiler community) has solved the problem of how to create a language that composes nicely. This is shown in Julia by interop between libraries in different stacks, like those implementing numbers, arrays, linear algebra etc. This is done through pattern matching and promotion between types (`Base.promote_rule`)\n",
    "\n",
    "\n",
    "So we can do things like build XLA backend easily and combine different ML tools.\n",
    "\n",
    "#### Multiple Dispatch\n",
    "[Julia - \"Methods\"](https://docs.julialang.org/en/v1/manual/methods/)\n",
    "\n",
    "> Julia allows the dispatch process to choose which of a function's methods to call based on the number of arguments given, and on the types of all of the function's arguments. This is different than traditional object-oriented languages, where dispatch occurs based only on the first argument, which often has a special argument syntax, and is sometimes implied rather than explicitly written as an argument.\n",
    "\n",
    "\n",
    "#### Promotion\n",
    "[Julia - \"Conversion and Promotion\"](https://docs.julialang.org/en/v1/manual/conversion-and-promotion/index.html)\n",
    "> In a sense, Julia falls into the \"no automatic promotion\" category: mathematical operators are just functions with special syntax, and the arguments of functions are never automatically converted. However, one may observe that applying mathematical operations to a wide variety of mixed argument types is just an extreme case of polymorphic multiple dispatch â€“ something which Julia's dispatch and type systems are particularly well-suited to handle. \"Automatic\" promotion of mathematical operands simply emerges as a special application: Julia comes with pre-defined catch-all dispatch rules for mathematical operators, invoked when no specific implementation exists for some combination of operand types. These catch-all rules first promote all operands to a common type using user-definable promotion rules, and then invoke a specialized implementation of the operator in question for the resulting values, now of the same type. User-defined types can easily participate in this promotion system by defining methods for conversion to and from other types, and providing a handful of promotion rules defining what types they should promote to when mixed with other types.\n",
    "\n",
    "Julia has two primary concepts here, conversion and promotion:\n",
    "\n",
    "> The convert function generally takes two arguments: the first is a type object and the second is a value to convert to that type. The returned value is the value converted to an instance of given type.\n",
    "\n",
    "> Promotion to a common \"greater\" type is performed in Julia by the promote function, which takes any number of arguments, and returns a tuple of the same number of values, converted to a common type, or throws an exception if promotion is not possible. The most common use case for promotion is to convert numeric arguments to a common type:\n",
    "\n",
    "The way I think about it is that conversion is used for object creation, and promotion is used in method defintions. i.e. if you try to create an object it will promote it automatically\n",
    "\n",
    "\n",
    "> Although one could, in principle, define methods for the promote function directly, this would require many redundant definitions for all possible permutations of argument types. Instead, the behavior of promote is defined in terms of an auxiliary function called promote_rule, which one can provide methods for. The promote_rule function takes a pair of type objects and returns another type object, such that instances of the argument types will be promoted to the returned type.\n",
    "\n",
    "You define functions mapping two types to their promoted type. \n",
    "\n",
    "### Tiark Rompf\n",
    "\n",
    "He has built a system for DSLs in Scala, called [\"Lightweight Modular Staging\" (LMS)](http://scala-lms.github.io/).\n",
    "\n",
    "General idea is that compiler becomes bottleneck between programmer and hardware, so we fix this by adding leveled compilers (Tensor/SQL layer => Array/loop layer => SIMD, etc). \n",
    "\n",
    "It's like Tensorflow but prettier, i.e. integrated better into the language. Uses `Rep[Integer]` to be the type of an `Integer` that is not yet computed. But it all works nicely at language level. \n",
    "\n",
    "It failed partially because *it wasn't in Python* so no one cared.\n",
    "\n",
    "So he made a version for python called [`snek-lms`](https://github.com/jmd1011/snek-LMS#lightweight-syntax-for-building-computation-graphs). It analyzes AST to turn control flow into graph.\n",
    "\n",
    "This:\n",
    "\n",
    "```python\n",
    "@lms\n",
    "def loop(n):\n",
    "    x = 0\n",
    "    while x < n:\n",
    "        x = x + 1\n",
    "    return x\n",
    "```\n",
    "\n",
    "Turns into this:\n",
    "\n",
    "```python\n",
    "def loop(n):\n",
    "    x = __new_var()\n",
    "    __assign(x, 0)\n",
    "\n",
    "    def cond$1():\n",
    "        return (__read(x) < n)\n",
    "\n",
    "    def body$1():\n",
    "        __assign(x, (__read(x) + 1))\n",
    "\n",
    "    __while(cond$1, body$1)\n",
    "\n",
    "    return __read(x)\n",
    "```\n",
    "\n",
    "Which creates this IR:\n",
    "\n",
    "\n",
    "```python\n",
    "[['let', x5, ['new_var']],\n",
    " ['let', x6, ['assign', x5, 0]],\n",
    " ['let', x7, ['while',\n",
    "    [['let', x7, ['read', x5]],\n",
    "     ['let', x8, ['<', x7, in]],\n",
    "     x8],\n",
    "    [['let', x7, ['read', x5]],\n",
    "     ['let', x8, ['+', x7, 1]],\n",
    "     ['let', x9, ['assign', x5, x8]],\n",
    "     None]]],\n",
    " ['let', x8, ['read', x5]], x8]\n",
    "```\n",
    "\n",
    "\n",
    "He is working witth Google on this for their [Tensorflow AutoGraph](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/autograph) project.\n",
    "\n",
    "He said that `snek-lms` is not TF or XLA specific and that Google folks working on autograph would likely be willing to accomdate non TF use cases as well. That the code is factored so it could be used without TF. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### TVM - Tianqi Chen\n",
    "TVM is OS ML stack out of Univ. of Washington.\n",
    "\n",
    "He has a colleague who developed a new ASIC and he wanted to collaborate to see how it performed. He didn't know where to plug this into the current ML stacks to compare it against other hardware.\n",
    "\n",
    "He had experience in MX Net where the idea is to have a high level computation graph, with expression as nodes. This abstractions fails for two reasons:\n",
    "\n",
    "1. Then each high level node needs mapping to low level. What if hardware doesn't support that operation?\n",
    "2. Operator fusion is essential for performance. But hand coding fusion is too time consuming/exponential number of combinations. \n",
    "\n",
    "So instead -> \"Learning based learning systems\"\n",
    "\n",
    "1. Start with tensor expression language\n",
    "2. Compute possible compile varients\n",
    "3. Use statistical cost model to figure out best. This involves actually testing different versions and gathering data.\n",
    "\n",
    "Compilation about an hour when we don't have existing statistics. \n",
    "\n",
    "### Glow - Facebook's IR\n",
    "\n",
    "Practical, simple, hardware backed IR. Only static shapes, no control flow, no branching. Unroll all loops before.\n",
    "\n",
    "ONNIXIFI is the bridge between PyTorch and Glow.  \n",
    "\n",
    "It supports many different proprietary hardware. All that is closed source. Used internally at FB to target this hardware from PyTorch. It also support backend specific instructions and partioning based of large graph into multiple nodes based on memory constraints.\n",
    "\n",
    "On the CPU it transforms to LLVM by using `libjit` a bunch of C++ operations on tensorors. For example this is [the tranpose](https://github.com/pytorch/glow/blob/312ce2113117bec464b4db84bc1dd2c6c2d7d982/lib/Backends/CPU/libjit/libjit.cpp#L435-L515):\n",
    "\n",
    "\n",
    "```c++\n",
    "void libjit_transpose_generic(const T *inW, T *outW, const size_t *idim,\n",
    "                              const size_t *odim, const size_t *shuffle,\n",
    "                              size_t numDims) {\n",
    "  // Transpose 2d matrices one tile at a time. This access pattern ensures\n",
    "  // that the whole tile is kept in L1 cache. When scanning the whole row at\n",
    "  // once we invalidate many cache lines when we touch a single column.\n",
    "  const unsigned tileSize = 64;\n",
    "\n",
    "  // Source coordinate.\n",
    "  size_t SC[5];\n",
    "\n",
    "  if (numDims == 5) {\n",
    "    for (size_t x = 0; x < odim[0]; x++)\n",
    "      for (size_t y = 0; y < odim[1]; y++)\n",
    "        for (size_t z = 0; z < odim[2]; z++)\n",
    "          for (size_t w = 0; w < odim[3]; w++)\n",
    "            for (size_t q = 0; q < odim[4]; q++) {\n",
    "              SC[shuffle[0]] = x;\n",
    "              SC[shuffle[1]] = y;\n",
    "              SC[shuffle[2]] = z;\n",
    "              SC[shuffle[3]] = w;\n",
    "              SC[shuffle[4]] = q;\n",
    "              outW[libjit_getXYZWQ(odim, x, y, z, w, q)] =\n",
    "                  inW[libjit_getXYZWQ(idim, SC[0], SC[1], SC[2], SC[3], SC[4])];\n",
    "            }\n",
    "    return;\n",
    "  }\n",
    "  ...\n",
    "```\n",
    "\n",
    "\n",
    "### XLA\n",
    "\n",
    "Why use XLA? Well it's the only way to run your code on a TPU.\n",
    "\n",
    "Lot's of thought goes into fusion, when to fuse and heauristics around this. It's a black box system though, you don't have any influence over how it fuses. Also has to figure out tiling and unrolling. \n",
    "\n",
    "\n",
    "\"Stencil\" is the intesting abstraction here. They break out all operations into:\n",
    "\n",
    "* memory access\n",
    "* computation\n",
    "\n",
    "and split these parts apart. So it becomes easier to compose computations and do fusion.\n",
    "\n",
    "However, all operations are hardcoded in core. They can only be added by modifying core.\n",
    "\n",
    "### Intel - nGraph\n",
    "It is an IR, a runtime, and a compiler. It does it all! \n",
    "\n",
    "### TACO\n",
    "\n",
    "Fusion is more important for sparse operations.\n",
    "\n",
    "The input is an enisum lik eoperation.\n",
    "\n",
    "Sparse arrays are implemented in different forms. Each \"level\" or dimension of the array can have a different representation like:\n",
    "\n",
    "* Dense\n",
    "* Hashed\n",
    "* ...\n",
    "\n",
    "Each of these represenations have different properties.\n",
    "\n",
    "Primary is difference between indexed vs. iterable layers. Some layers have O(1) indexing, others have O(1) iteration, which require different code to be generated.\n",
    "\n",
    "More details are in [\"Format Abstraction for Sparse Tensor Algebra Compilers\"](http://tensor-compiler.org/chou-oopsla18-taco-formats.pdf) paper.\n",
    "\n",
    "\n",
    "It also has a policy language specify which path to take, with different types of heuristics.\n",
    "\n",
    "### PlaidML\n",
    "\n",
    "By Intel, used on top of ngraph, but also goes to other backends.\n",
    "\n",
    "Uses polyhedral compilation and the primary abstraction is a \"block\". Block nesting is the same as dimension lifitng. Also similar to Dask's abstraction level. \n",
    "\n",
    "The right granularity for these IRs is graphs of reordable loops \"loop nest\". \n",
    "\n",
    "### Nvidia - Diesel\n",
    "\n",
    "We need new compiler for parallel graphs and parallel hardware. \n",
    "\n",
    "They are \"doing what everyone is doing\", which is JITing once they know dimensions. \n",
    "\n",
    "Use a \"l\n",
    "\n",
    "## Conversations\n",
    "\n",
    "### Chris Lattner\n",
    "TVM has great ideas but is active research project and isn't stable. Compile times are too long.\n",
    "\n",
    "All systems demonstrated here are good but they are usually serving short term business goals. So they don't design for the long hall. That's what he is trying to do.\n",
    "\n",
    "Both Tensorflow and TVM are also showing their warts. XLA is limited to static shapes. Tensorflow has many different frontends and backends. It's a mess internally trying to implement conversions between them. \n",
    "\n",
    "\n",
    "## Implications \n",
    "\n",
    "\n",
    "\n",
    "### ML in Python\n",
    "\n",
    "Python is **the** target for all of these tools. However they often view it as a sort of last-part dumping ground. It seems that providing Pythonic APIs is not top priority or thinking about how Python user's interact with their tool. \n",
    "\n",
    "---\n",
    "\n",
    "Julia's take is that it's nice to write ML all inside one language, without switching between them. Allows sharing of tools and movement up and down the stack more reasily.\n",
    "\n",
    "What is Python's \"take\" here?\n",
    "\n",
    "To put a positive spin on it, we could say that in Julia, it tries to *own* the computation space. As in, it wants control and ownership over what's going on. If we take the opposite position, that would be very unopinionated about where/who does the computataion (fortran compiled function, tensorflow C++ executor, AOT compilation), but just let you stitch those different systems together and offer helpful abstractions over them.\n",
    "\n",
    "---\n",
    "\n",
    "After Tiark Rompf's talk I asked about integrating LMS with Python more directly and how this work could impact the broader community. He has no interest of attending PyCon or interacting with Python community to talk about language level changes to support this kind of thing.\n",
    "\n",
    "His advice was to get google people working on autograph to do this. Maybe by creating a ML for Python group and having people from different industry as a part of it to inform language development.\n",
    "\n",
    "\n",
    "\n",
    "### `uarray`\n",
    "\n",
    "From Julia, both conversion and promotion are useful concepts to copy hopefully.\n",
    "  * Conversion: This would be useful to get remove some of the noisy casting between things like \"array\" and \"MoA\" etc.\n",
    "  * Promotion: It's possible we could use this for AST creation. For example, if we have `Natural(a) + Natural(b)` if a is an integer and b is an AST node the integer should be promoted to an ast node and then the conversion would happen. So this would be instead of use (problematic) `to_ast` function which is hard to understand when/what it should convert. This reframes the question by removing that function, and instead rely on conversion semantics to think about compilation.\n",
    "  \n",
    "---\n",
    "\n",
    "\n",
    "We could use `snek-lms` instead of creating lazy ndarray to create graph. Two reasons:\n",
    "\n",
    "1. Woud allow us to support dynamic control flow/conditionals in native python syntax\n",
    "2. might be easier to extract original line numbers of python code to tie them to output for debugging purposes.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "There are crazy amounts of work to compile deep learning to lowest levels, including quantization (choosing what size weights to use), tiling (so that compute/memory access is overlapping as much as possible). We should stick to the high level, as much as possible! This means offloading as much work as we can, and staying away from MoA pipelines (or at least not neccesarily making them primary). Many pipeliens do optimizations at whole operation level (i.e. transpose of matrix multiplication) so we need to pass it to them in this form if they are to optimize it.\n",
    "\n",
    "We should target `ngraph`, `PlaidML`, Taco, XLA, TVM.\n",
    "\n",
    "---\n",
    "\n",
    "To properly target sparse matrices we need to adopt TACO mentality. This means talking about arrays as either allowing direct indexing OR iterationg, by dimension. This is fundamentally different than thinking about them exclusively as O(1) mapping from indices to values. \n",
    "\n",
    "\"Highest level\" representation is really this iterable idea. i.e. we can build performant iteration on top of indexing, but not vice versa. So iteration, by dimension, is \"lowest\"  type of tensor.\n",
    "\n",
    "We need to figure out how to integrate this at API level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
